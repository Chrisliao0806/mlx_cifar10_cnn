# ğŸš€ CIFAR-10 Image Classification with MLX

<p align="center">
  <img src="https://img.shields.io/badge/Apple%20Silicon-M4%20Max-black?style=for-the-badge&logo=apple&logoColor=white" alt="Apple Silicon"/>
  <img src="https://img.shields.io/badge/MLX-Framework-orange?style=for-the-badge" alt="MLX"/>
  <img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/CIFAR--10-Dataset-green?style=for-the-badge" alt="CIFAR-10"/>
</p>

A high-performance deep learning implementation for CIFAR-10 image classification, leveraging **Apple's MLX framework** optimized for Apple Silicon. This project demonstrates a custom CNN architecture enhanced with **Self-Attention mechanisms**, achieving impressive results with native GPU acceleration on M-series chips.

---

## âœ¨ Highlights

- ğŸ **Native Apple Silicon Acceleration** â€” Built with MLX for optimal performance on M1/M2/M3/M4 chips
- ğŸ§  **Hybrid CNN + Self-Attention Architecture** â€” Combines the power of convolutional layers with attention mechanisms
- âš¡ **Blazing Fast Training** â€” Optimized for Apple's unified memory architecture
- ğŸ“Š **Complete Training Pipeline** â€” Data augmentation, cosine annealing, and comprehensive metrics tracking
- ğŸ¯ **Production-Ready Code** â€” Clean, modular, and well-documented implementation

---

## ğŸ—ï¸ Model Architecture

```
CifarAttentionNet
â”œâ”€â”€ Prep Layer: Conv2D(3 â†’ 64) + BatchNorm + ReLU
â”œâ”€â”€ Layer 1: Conv2D(64 â†’ 128) + MaxPool + Residual Block
â”œâ”€â”€ Layer 2: Conv2D(128 â†’ 256) + MaxPool + Self-Attention (4 heads)
â”œâ”€â”€ Layer 3: Conv2D(256 â†’ 512) + MaxPool + Residual Block
â””â”€â”€ Classifier: Global MaxPool â†’ Linear(512 â†’ 10)
```

### Key Components

| Component | Description |
|-----------|-------------|
| **ConvBlock** | Conv2D â†’ BatchNorm â†’ ReLU with optional MaxPooling |
| **SelfAttention** | Multi-head self-attention with 4 heads for capturing global dependencies |
| **Residual Connections** | Skip connections for stable gradient flow |

---

## ğŸ”§ Technical Specifications

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Batch Size | 128 |
| Epochs | 10 |
| Base Learning Rate | 0.001 |
| Optimizer | AdamW |
| Weight Decay | 1e-4 |
| LR Schedule | Cosine Annealing |

### Data Augmentation

- Random Crop (32Ã—32 with padding=4)
- Random Horizontal Flip
- Normalization (Î¼=[0.4914, 0.4822, 0.4465], Ïƒ=[0.2023, 0.1994, 0.2010])

---

## ğŸ’» Hardware & Environment

Developed and tested on:

| Spec | Details |
|------|---------|
| **Device** | MacBook Pro |
| **Chip** | Apple M4 Max |
| **RAM** | 36GB Unified Memory |
| **Framework** | MLX (Apple's ML Framework) |

---

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install mlx torch torchvision numpy matplotlib tqdm
```

### Run Training

Open `cifar10_mlx.ipynb` in Jupyter Notebook or VS Code and run all cells:

```bash
jupyter notebook cifar10_mlx.ipynb
```

The CIFAR-10 dataset will be automatically downloaded on first run.

---

## ğŸ“ Project Structure

```
mlx_cifar10_cnn/
â”œâ”€â”€ cifar10_mlx.ipynb    # Main training notebook
â”œâ”€â”€ data/                 # CIFAR-10 dataset (auto-downloaded)
â”‚   â””â”€â”€ cifar-10-batches-py/
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ“ˆ Training Features

- **Real-time Progress Tracking** â€” tqdm progress bars with live loss/accuracy updates
- **Learning Rate Visualization** â€” Dynamic LR shown during training
- **History Plotting** â€” Automatic generation of loss/accuracy curves
- **Validation Metrics** â€” Comprehensive evaluation on test set after each epoch

---

## ğŸ¯ Why MLX?

[MLX](https://github.com/ml-explore/mlx) is Apple's machine learning framework designed specifically for Apple Silicon. Key advantages:

1. **Unified Memory** â€” No CPUâ†”GPU data transfer overhead
2. **Lazy Evaluation** â€” Efficient computation graph execution
3. **NumPy-like API** â€” Familiar and intuitive syntax
4. **Native Performance** â€” Optimized for M-series Neural Engine and GPU

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Apple MLX Team](https://github.com/ml-explore/mlx) for the amazing framework
- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html) by Alex Krizhevsky
- PyTorch team for data loading utilities

---

<p align="center">
  <b>Built with â¤ï¸ on Apple Silicon</b>
</p>