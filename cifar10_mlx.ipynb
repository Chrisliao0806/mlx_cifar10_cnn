{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10   \n",
    "BASE_LR = 0.001  \n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get cifar 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_dataloaders(batch_size=128):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cifar_preview(dataloader, rows=4, cols=4):\n",
    "    \"\"\"\n",
    "    Display a grid of CIFAR-10 images, each with its own label\n",
    "    \"\"\"\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "                'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    # 1. get a Batch\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    # 2. define normalization parameters (must match Transforms)\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).reshape(3, 1, 1)\n",
    "    std = torch.tensor([0.2023, 0.1994, 0.2010]).reshape(3, 1, 1)\n",
    "\n",
    "    # figsize controls the size of the entire image, make it bigger to avoid crowding\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "\n",
    "    # Flatten the multi-dimensional array for easier looping (e.g., 4x4 -> 16 axes)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if i < len(images):\n",
    "            img = images[i]\n",
    "            label_idx = labels[i].item()\n",
    "            label_name = classes[label_idx]\n",
    "\n",
    "            img = img * std + mean\n",
    "            \n",
    "            # Transpose dimensions: (C, H, W) -> (H, W, C)\n",
    "            npimg = img.numpy().transpose(1, 2, 0)\n",
    "            \n",
    "            # Ensure values are within [0, 1] to avoid matplotlib warnings or display issues\n",
    "            npimg = np.clip(npimg, 0, 1)\n",
    "\n",
    "            ax.imshow(npimg)\n",
    "            ax.set_title(label_name, fontsize=12, color='blue')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Data...\")\n",
    "train_loader, test_loader = get_cifar10_dataloaders(batch_size=BATCH_SIZE)\n",
    "show_cifar_preview(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dims, heads=4):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dims // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dims, dims * 3, bias=False)\n",
    "        self.proj = nn.Linear(dims, dims)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x_flat = x.reshape(B, -1, C)\n",
    "        N = x_flat.shape[1]\n",
    "        \n",
    "        qkv = self.qkv(x_flat)\n",
    "        q, k, v = mx.split(qkv, 3, axis=-1)\n",
    "        \n",
    "        head_dim = C // self.heads\n",
    "        q = q.reshape(B, N, self.heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(B, N, self.heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        attn = mx.softmax((q @ k.transpose(0, 1, 3, 2)) * self.scale, axis=-1)\n",
    "        out = (attn @ v).transpose(0, 2, 1, 3).reshape(B, N, C)\n",
    "        \n",
    "        out = self.proj(out)\n",
    "        return out.reshape(B, H, W, C)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm(out_channels)\n",
    "        self.pool = pool\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = nn.relu(x)\n",
    "        if self.pool:\n",
    "            x = nn.MaxPool2d(kernel_size=2, stride=2)(x)\n",
    "        return x\n",
    "\n",
    "class CifarAttentionNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.prep = ConvBlock(3, 64)\n",
    "        \n",
    "        self.layer1_conv = ConvBlock(64, 128, pool=True)\n",
    "        self.layer1_res = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n",
    "        \n",
    "        self.layer2_conv = ConvBlock(128, 256, pool=True)\n",
    "        self.attention = SelfAttention(dims=256, heads=4)\n",
    "        \n",
    "        self.layer3_conv = ConvBlock(256, 512, pool=True)\n",
    "        self.layer3_res = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.prep(x)\n",
    "        x = self.layer1_conv(x)\n",
    "        x = x + self.layer1_res(x)\n",
    "        x = self.layer2_conv(x)\n",
    "        x = x + self.attention(x)\n",
    "        x = self.layer3_conv(x)\n",
    "        x = x + self.layer3_res(x)\n",
    "        x = mx.max(x, axis=[1, 2])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define loss function and plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, X, y):\n",
    "    logits = model(X)\n",
    "    return nn.losses.cross_entropy(logits, y, reduction=\"mean\")\n",
    "\n",
    "def eval_fn(model, loader):\n",
    "    \"\"\"calculate loss and accuracy on validation set\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for X, y in loader:\n",
    "        X = mx.array(X.numpy()).transpose(0, 2, 3, 1)\n",
    "        y = mx.array(y.numpy())\n",
    "        logits = model(X)\n",
    "        \n",
    "        # 計算 loss\n",
    "        loss = nn.losses.cross_entropy(logits, y, reduction=\"mean\")\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        \n",
    "        # 計算 accuracy\n",
    "        preds = mx.argmax(logits, axis=1)\n",
    "        correct += mx.sum(preds == y).item()\n",
    "        total += y.shape[0]\n",
    "    \n",
    "    return correct / total, total_loss / steps\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history['train_acc']\n",
    "    val_acc = history['val_acc']\n",
    "    loss = history['train_loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b-', label='Training Acc')\n",
    "    plt.plot(epochs, val_acc, 'r-', label='Validation Acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    print(\"Initializing Model...\")\n",
    "    model = CifarAttentionNet()\n",
    "    mx.eval(model.parameters())\n",
    "    \n",
    "    optimizer = optim.AdamW(learning_rate=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    # use nn.value_and_grad to handle nn.Module\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "\n",
    "    def step(X, y):\n",
    "        loss, grads = loss_and_grad_fn(model, X, y)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        # Cosine Annealing\n",
    "        lr = 0.5 * BASE_LR * (1 + math.cos(math.pi * epoch / EPOCHS))\n",
    "        optimizer.learning_rate = lr\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1:02d}/{EPOCHS}\")\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for X, y in pbar:\n",
    "            X = mx.array(X.numpy()).transpose(0, 2, 3, 1)\n",
    "            y = mx.array(y.numpy())\n",
    "            \n",
    "            loss = step(X, y)\n",
    "            mx.eval(model.state, optimizer.state)\n",
    "            \n",
    "            # calulate training accuracy\n",
    "            logits = model(X)\n",
    "            preds = mx.argmax(logits, axis=1)\n",
    "            batch_correct = mx.sum(preds == y).item()\n",
    "            batch_size = y.shape[0]\n",
    "            \n",
    "            l = loss.item()\n",
    "            epoch_loss += l\n",
    "            epoch_correct += batch_correct\n",
    "            epoch_total += batch_size\n",
    "            steps += 1\n",
    "            \n",
    "            pbar.set_postfix(loss=f\"{l:.4f}\", acc=f\"{batch_correct/batch_size:.4f}\", lr=f\"{lr:.5f}\")\n",
    "        \n",
    "        avg_train_loss = epoch_loss / steps\n",
    "        avg_train_acc = epoch_correct / epoch_total\n",
    "        \n",
    "        # calculate accuracy and loss on validation set\n",
    "        model.eval()\n",
    "        val_acc, val_loss = eval_fn(model, test_loader)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(avg_train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"  └─ Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinal Test Accuracy: {history['val_acc'][-1]:.2%}\")\n",
    "    plot_history(history)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
